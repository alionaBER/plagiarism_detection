{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Containment\n",
    "\n",
    "In this notebook, you'll implement a containment function that looks at a source and answer text and returns a *normalized* value that represents the similarity between those two texts based on their n-gram intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram counts\n",
    "\n",
    "One of the first things you'll need to do is to count up the occurrences of n-grams in your text data. To convert a set of text data into a matrix of counts, you can use a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "Below, you can set a value for n and use a CountVectorizer is used to count up the n-gram occurrences. In the next cell, we'll see that the CountVectorizer constructs a vocabulary, and later, we'll look at the matrix of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 5, 'is': 2, 'an': 0, 'answer': 1, 'text': 4, 'source': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "a_text = \"This is an answer text\"\n",
    "s_text = \"This is a source text\"\n",
    "\n",
    "# set n\n",
    "n = 1\n",
    "\n",
    "# instantiate an ngram counter\n",
    "counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
    "\n",
    "# create a dictionary of n-grams by calling `.fit`\n",
    "vocab2int = counts.fit([a_text, s_text]).vocabulary_\n",
    "\n",
    "# print dictionary of words:index\n",
    "print(vocab2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Create a vocabulary for 2-grams (aka \"bigrams\")\n",
    "\n",
    "Create a `CountVectorizer`, `counts_2grams`, and fit it to our text data. Print out the resultant vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this is': 6, 'is an': 4, 'an answer': 1, 'answer text': 2, 'is a': 3, 'a source': 0, 'source text': 5}\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary for 2-grams\n",
    "# set n\n",
    "n = 2\n",
    "counts_2grams = CountVectorizer(analyzer='word', ngram_range=(n,n), token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "# create a dictionary of n-grams by calling `.fit`\n",
    "vocab2int_2grams = counts_2grams.fit([a_text, s_text]).vocabulary_\n",
    "\n",
    "# print dictionary of words:index\n",
    "print(vocab2int_2grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes up a word?\n",
    "\n",
    "You'll note that the word \"a\" does not appear in the vocabulary. And also that the words have been converted to lowercase. When `CountVectorizer` is passed `analyzer='word'` it defines a word as *two or more* characters and so it ignores uni-character words. In a lot of text analysis, single characters are often irrelevant to the meaning of a passage, so leaving them out of a vocabulary is often desired behavior. \n",
    "\n",
    "For our purposes, this default behavior will work well; we don't need uni-character words to determine cases of plagiarism, but you may still want to experiment with uni-character counts.\n",
    "\n",
    "> If you *do* want to include single characters as words, you can choose to do so by adding one more argument when creating the `CountVectorizer`; pass in the definition of a token, `token_pattern = r\"(?u)\\b\\w+\\b\"`. \n",
    "\n",
    "This regular expression defines a word as one or more characters. If you want to learn more about this vectorizer, I suggest reading through the [source code](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L664), which is well documented.\n",
    "\n",
    "**Next, let's fit our `CountVectorizer` to all of our text data to make an array of n-gram counts!**\n",
    "\n",
    "The below code, assumes that `counts` is our `CountVectorizer` for the n-gram size we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1]\n",
      " [1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# create array of n-gram counts for the answer and source text\n",
    "ngrams = counts.fit_transform([a_text, s_text])\n",
    "\n",
    "# row = the 2 texts and column = indexed vocab terms (as mapped above)\n",
    "# ex. column 0 = 'an', col 1 = 'answer'.. col 4 = 'text'\n",
    "ngram_array = ngrams.toarray()\n",
    "print(ngram_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the top row indicates the n-gram counts for the answer text `a_text`, and the second row indicates those for the source text `s_text`. If they have n-grams in common, you can see this by looking at the column values. For example they both have one \"is\" (column 2) and \"text\" (column 4) and \"this\" (column 5).\n",
    "\n",
    "```\n",
    "[[1 1 1 0 1 1]    =   an  answer  [is]  ______  [text] [this]\n",
    " [0 0 1 1 1 1]]   =   __  ______  [is]  source  [text] [this]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Calculate containment values\n",
    "\n",
    "Assume your function takes in an `ngram_array` just like that generated above, for an answer text (row 0) and a source text (row 1). Using just this information, calculate the containment between the two texts. As before, it's okay to ignore the uni-character words.\n",
    "\n",
    "To calculate the containment:\n",
    "1. Calculate the n-gram **intersection** between the answer and source text.\n",
    "2. Add up the number of common terms.\n",
    "3. Normalize by dividing the value in step 2 by the number of n-grams in the answer text.\n",
    "\n",
    "The complete equation is:\n",
    "\n",
    "$$ \\frac{\\sum{count(\\text{ngram}_{A}) \\cap count(\\text{ngram}_{S})}}{\\sum{count(\\text{ngram}_{A})}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containment(ngram_array):\n",
    "    ''' Containment is a measure of text similarity. It is the normalized, \n",
    "       intersection of ngram word counts in two texts.\n",
    "       :param ngram_array: an array of ngram counts for an answer and source text.\n",
    "       :return: a normalized containment value.'''\n",
    "    \n",
    "    \n",
    "    # your code here\n",
    "    interseption_ngrams_count = np.logical_and(ngram_array[0,], ngram_array[1,]).sum()\n",
    "    a_ngrams_count = ngram_array[0,].sum()\n",
    "    return interseption_ngrams_count / a_ngrams_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Containment:  0.6\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# test out your code\n",
    "containment_val = containment(ngrams.toarray())\n",
    "\n",
    "print('Containment: ', containment_val)\n",
    "\n",
    "# note that for the given texts, and n = 1\n",
    "# the containment value should be 3/5 or 0.6\n",
    "assert containment_val==0.6, 'Unexpected containment value for n=1.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Containment for n=2 :  0.25\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# test for n = 2\n",
    "counts_2grams = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "bigram_counts = counts_2grams.fit_transform([a_text, s_text])\n",
    "\n",
    "# calculate containment\n",
    "containment_val = containment(bigram_counts.toarray())\n",
    "\n",
    "print('Containment for n=2 : ', containment_val)\n",
    "\n",
    "# the containment value should be 1/4 or 0.25\n",
    "assert containment_val==0.25, 'Unexpected containment value for n=2.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend trying out different phrases, and different values of n. What happens if you count for uni-character words? What if you make the sentences much larger?\n",
    "\n",
    "I find that the best way to understand a new concept is to think about how it might be applied in a variety of different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longer tests that considered plagiarism by Bowdoin\n",
    "(examples sourced from https://www.bowdoin.edu/dean-of-students/judicial-board/academic-honesty-and-plagiarism/examples.html)\n",
    "\n",
    "The containment detects similarity with a single source and depend on the length of original plagiarised text. A direct plagiarism may stay undetected if it is comparatively short in an analysed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Containment:  0.45652173913\n"
     ]
    }
   ],
   "source": [
    "a_text_long = \"Only two years later, all these friendly Sioux were suddenly plunged into new conditions, including starvation, martial law on all their reservations, and constant urging by their friends and relations to join in warfare against the treacherous government that had kept faith with neither friend nor foe.\"\n",
    "s_text_long = \"In ages which have no record these islands were the home of millions of Contrast the condition into which all these friendly Indians are suddenly plunged now, with their condition only two years previous: martial law now in force on all their reservations; themselves in danger of starvation, and constantly exposed to the influence of emissaries from their friends and relations, urging them to join in fighting this treacherous government that had kept faith with nobody--neither with friend nor with foe.\"\n",
    "\n",
    "n = 2\n",
    "# instantiate an ngram counter\n",
    "counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
    "\n",
    "# create array of n-gram counts for the answer and source text\n",
    "ngrams = counts.fit_transform([a_text_long, s_text_long])\n",
    "\n",
    "containment_val = containment(ngrams.toarray())\n",
    "print('Containment: ', containment_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
